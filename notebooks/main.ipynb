{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Output CNN for Speech Recognition and Gender Classification\n",
        "\n",
        "This notebook implements a **Multi-Output CNN** that predicts both digit and gender simultaneously:\n",
        "\n",
        "## ðŸ§  Methodology\n",
        "1. **Feature Extraction**\n",
        "   - Extract Mel-Frequency Cepstral Coefficients (MFCCs) from audio recordings using Librosa\n",
        "   - Standardize input shape by padding/truncating sequences to a fixed length\n",
        "\n",
        "2. **Data Preparation**\n",
        "   - Convert audio into numpy arrays of shape (n_mfcc, time, 1)\n",
        "   - Encode labels:\n",
        "     - Gender â†’ one-hot vector ([1,0] = male, [0,1] = female)\n",
        "     - Digit â†’ one-hot vector of length 10\n",
        "\n",
        "3. **Model Architecture (Multi-Output CNN)**\n",
        "   - Convolutional Neural Network (CNN) extracts timeâ€“frequency patterns\n",
        "   - Shared convolutional layers â†’ two output branches:\n",
        "     - Gender classifier (softmax over 2 classes)\n",
        "     - Digit classifier (softmax over 10 classes)\n",
        "\n",
        "4. **Training & Evaluation**\n",
        "   - Train/test split = 70/30\n",
        "   - Loss function = categorical crossentropy (for both tasks)\n",
        "   - Metrics = accuracy, precision, recall, F1-score\n",
        "   - Visualization of training history (accuracy/loss curves)\n",
        "\n",
        "## Dataset Structure\n",
        "```\n",
        "Dataset/\n",
        "   d0/ (digit 0)\n",
        "      male/   â†’ male speakers saying \"zero\"\n",
        "      female/ â†’ female speakers saying \"zero\"\n",
        "   d1/ (digit 1)\n",
        "      male/   â†’ male speakers saying \"one\"\n",
        "      female/ â†’ female speakers saying \"one\"\n",
        "   ...\n",
        "   d9/ (digit 9)\n",
        "      male/   â†’ male speakers saying \"nine\"\n",
        "      female/ â†’ female speakers saying \"nine\"\n",
        "```\n",
        "\n",
        "**Note**: We predict BOTH digit (0-9) AND gender (male/female) simultaneously using a Multi-Output CNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Project imports\n",
        "from prepare_dataset import load_dataset\n",
        "from train_model import build_model\n",
        "from extract_features import extract_mfcc\n",
        "from utils import plot_history\n",
        "\n",
        "# Plot style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print('âœ… Imports complete. Ready to run the pipeline.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_PATH = 'dataset/'  # must match src expectations\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "print('DATASET_PATH:', DATASET_PATH)\n",
        "print('EPOCHS:', EPOCHS, 'BATCH_SIZE:', BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Dataset Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "digits = [f'd{i}' for i in range(10)]\n",
        "genders = ['male', 'female']\n",
        "\n",
        "digit_counts = {d: 0 for d in digits}\n",
        "gender_counts = {g: 0 for g in genders}\n",
        "total_files = 0\n",
        "\n",
        "for d in digits:\n",
        "    digit_dir = os.path.join(DATASET_PATH, d)\n",
        "    if not os.path.isdir(digit_dir):\n",
        "        continue\n",
        "    for g in genders:\n",
        "        gdir = os.path.join(digit_dir, g)\n",
        "        if not os.path.isdir(gdir):\n",
        "            continue\n",
        "        wavs = [f for f in os.listdir(gdir) if f.lower().endswith('.wav')]\n",
        "        digit_counts[d] += len(wavs)\n",
        "        gender_counts[g] += len(wavs)\n",
        "        total_files += len(wavs)\n",
        "\n",
        "print('Total audio files:', total_files)\n",
        "print('Gender counts:', gender_counts)\n",
        "print('Digit counts:', digit_counts)\n",
        "\n",
        "# Visualization\n",
        "if total_files > 0:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Gender pie\n",
        "    axes[0].pie([gender_counts.get('male', 0), gender_counts.get('female', 0)],\n",
        "               labels=['Male', 'Female'], autopct='%1.1f%%',\n",
        "               colors=['lightblue', 'lightpink'], startangle=90)\n",
        "    axes[0].set_title('Gender Distribution')\n",
        "    \n",
        "    # Digit bar\n",
        "    d_labels = list(digit_counts.keys())\n",
        "    d_values = [digit_counts[k] for k in d_labels]\n",
        "    bars = axes[1].bar(d_labels, d_values, alpha=0.8, color='lightgreen')\n",
        "    axes[1].set_title('Digit Distribution')\n",
        "    axes[1].set_ylabel('Samples')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    for bar, count in zip(bars, d_values):\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, str(count),\n",
        "                     ha='center', va='bottom')\n",
        "    \n",
        "    # Summary\n",
        "    axes[2].axis('off')\n",
        "    axes[2].text(0.5, 0.7, f'Total Files: {total_files}', ha='center', va='center',\n",
        "                 fontsize=14, fontweight='bold', transform=axes[2].transAxes)\n",
        "    axes[2].text(0.5, 0.5, f\"Male: {gender_counts.get('male', 0)}\", ha='center', va='center',\n",
        "                 fontsize=12, transform=axes[2].transAxes)\n",
        "    axes[2].text(0.5, 0.35, f\"Female: {gender_counts.get('female', 0)}\", ha='center', va='center',\n",
        "                 fontsize=12, transform=axes[2].transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No files found. Please ensure dataset is available at', DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. MFCC Feature Extraction Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = None\n",
        "for d in [f'd{i}' for i in range(10)]:\n",
        "    male_dir = os.path.join(DATASET_PATH, d, 'male')\n",
        "    female_dir = os.path.join(DATASET_PATH, d, 'female')\n",
        "    for gdir in [male_dir, female_dir]:\n",
        "        if os.path.isdir(gdir):\n",
        "            wavs = [f for f in os.listdir(gdir) if f.lower().endswith('.wav')]\n",
        "            if wavs:\n",
        "                sample_file = os.path.join(gdir, wavs[0])\n",
        "                break\n",
        "    if sample_file:\n",
        "        break\n",
        "\n",
        "if sample_file is None:\n",
        "    print('âŒ No sample audio found in dataset. Skipping demo.')\n",
        "else:\n",
        "    print('Sample file:', sample_file)\n",
        "    mfcc = extract_mfcc(sample_file)\n",
        "    if mfcc is None:\n",
        "        print('Failed to extract MFCC for sample.')\n",
        "    else:\n",
        "        print('MFCC shape:', mfcc.shape)\n",
        "        # Load raw audio for waveform\n",
        "        y, sr = librosa.load(sample_file, sr=16000)\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "        # Waveform\n",
        "        axes[0].plot(y)\n",
        "        axes[0].set_title('Waveform')\n",
        "        axes[0].set_xlabel('Samples')\n",
        "        axes[0].set_ylabel('Amplitude')\n",
        "        # MFCC heatmap\n",
        "        img = axes[1].imshow(mfcc, aspect='auto', origin='lower', cmap='viridis')\n",
        "        axes[1].set_title('MFCC (normalized)')\n",
        "        axes[1].set_xlabel('Time Frames')\n",
        "        axes[1].set_ylabel('MFCC Coefficients')\n",
        "        fig.colorbar(img, ax=axes[1], shrink=0.8)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Load Dataset and Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (70/30 split is handled inside load_dataset)\n",
        "X_train, X_test, y_train, y_test = load_dataset(DATASET_PATH)\n",
        "\n",
        "print('X_train:', X_train.shape, 'X_test:', X_test.shape)\n",
        "print('y_train (digit, gender):', [arr.shape for arr in y_train])\n",
        "print('y_test (digit, gender):', [arr.shape for arr in y_test])\n",
        "\n",
        "# Build model\n",
        "input_shape = X_train.shape[1:]\n",
        "model = build_model(input_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Show training curves\n",
        "plot_history(history)\n",
        "\n",
        "# Save training curves\n",
        "h = history.history\n",
        "\n",
        "# Accuracy plots\n",
        "plt.figure(figsize=(8,5))\n",
        "if 'digit_output_accuracy' in h:\n",
        "    plt.plot(h['digit_output_accuracy'], label='train_digit')\n",
        "if 'val_digit_output_accuracy' in h:\n",
        "    plt.plot(h['val_digit_output_accuracy'], label='val_digit')\n",
        "if 'gender_output_accuracy' in h:\n",
        "    plt.plot(h['gender_output_accuracy'], label='train_gender', linestyle='--')\n",
        "if 'val_gender_output_accuracy' in h:\n",
        "    plt.plot(h['val_gender_output_accuracy'], label='val_gender', linestyle='--')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('plots', 'training_accuracy.png'))\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.figure(figsize=(8,5))\n",
        "if 'loss' in h:\n",
        "    plt.plot(h['loss'], label='train_loss')\n",
        "if 'val_loss' in h:\n",
        "    plt.plot(h['val_loss'], label='val_loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('plots', 'training_loss.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions (model has two outputs)\n",
        "preds = model.predict(X_test)\n",
        "if isinstance(preds, list) and len(preds) == 2:\n",
        "    y_pred_digit_proba, y_pred_gender_proba = preds\n",
        "else:\n",
        "    raise RuntimeError('Model did not return two outputs on predict()')\n",
        "\n",
        "y_true_digit = np.argmax(y_test[0], axis=1)\n",
        "y_true_gender = np.argmax(y_test[1], axis=1)\n",
        "y_pred_digit = np.argmax(y_pred_digit_proba, axis=1)\n",
        "y_pred_gender = np.argmax(y_pred_gender_proba, axis=1)\n",
        "\n",
        "# Classification reports\n",
        "digit_report = classification_report(y_true_digit, y_pred_digit)\n",
        "gender_report = classification_report(y_true_gender, y_pred_gender)\n",
        "print('Digit Classification Report:\\\\n', digit_report)\n",
        "print('Gender Classification Report:\\\\n', gender_report)\n",
        "\n",
        "# Save reports\n",
        "with open(os.path.join('plots', 'classification_reports.txt'), 'w') as f:\n",
        "    f.write('Digit Classification Report:\\\\n')\n",
        "    f.write(digit_report + '\\\\n\\\\n')\n",
        "    f.write('Gender Classification Report:\\\\n')\n",
        "    f.write(gender_report + '\\\\n')\n",
        "\n",
        "# Confusion matrix - digits\n",
        "cm_digits = confusion_matrix(y_true_digit, y_pred_digit)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(cm_digits, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Digits')\n",
        "plt.colorbar()\n",
        "ticks = np.arange(cm_digits.shape[0])\n",
        "plt.xticks(ticks, ticks)\n",
        "plt.yticks(ticks, ticks)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "thresh = cm_digits.max() / 2.0\n",
        "for i in range(cm_digits.shape[0]):\n",
        "    for j in range(cm_digits.shape[1]):\n",
        "        plt.text(j, i, format(cm_digits[i, j], 'd'),\n",
        "                 horizontalalignment='center',\n",
        "                 color='white' if cm_digits[i, j] > thresh else 'black')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('plots', 'confusion_digits.png'))\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix - gender\n",
        "cm_gender = confusion_matrix(y_true_gender, y_pred_gender)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm_gender, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Gender')\n",
        "plt.colorbar()\n",
        "plt.xticks([0,1], [0,1])\n",
        "plt.yticks([0,1], [0,1])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "thresh = cm_gender.max() / 2.0\n",
        "for i in range(cm_gender.shape[0]):\n",
        "    for j in range(cm_gender.shape[1]):\n",
        "        plt.text(j, i, format(cm_gender[i, j], 'd'),\n",
        "                 horizontalalignment='center',\n",
        "                 color='white' if cm_gender[i, j] > thresh else 'black')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('plots', 'confusion_gender.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = os.path.join('models', 'gender_digit_classifier.h5')\n",
        "model.save(model_path)\n",
        "print('âœ… Model saved at:', model_path)\n",
        "print('You can now use src/interface.py or src/gender_classifier.py for inference.')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
